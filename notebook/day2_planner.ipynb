{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day2 （環境から計画を立てる）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは動的計画法(Dynamic　Programing: DP)を扱う。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "sys.path.append('../../baby-steps-of-rl-ja/DP/')\n",
    "\n",
    "from environment import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day1おさらい\n",
    "\n",
    "- 強化学習はマルコフ決定過程を仮定し、その元で報酬の総和を最大化する方策をもつエージェントをもとめる問題\n",
    "    - マルコフ決定過程は以下の(S, A, T, R)の組である\n",
    "        - S: 状態全体の集合\n",
    "        - A: アクション全体の集合\n",
    "        - T: S×A×S -> [0, 1] 状態とアクションから次の状態への遷移確率を返す関数\n",
    "        - R: S×A×S -> 実数全体 状態, アクション, 遷移状態の状態から報酬を返す関数\n",
    "\n",
    "\n",
    "- Environmentクラス\n",
    "    - 迷路環境クラス\n",
    "    - State, Actionを内部に持つ\n",
    "    - Environment.step()メソッドにアクションを渡すことで、内部で遷移確率T(s'| s, a)を計算し、Tに応じた(状態遷移/rewardの計算/終了判定)をする。\n",
    "    \n",
    "- Agentクラス\n",
    "    - Environmentオブジェクトを持ち、stateに応じたアクションを返すpolicyメソッドを内部で持つ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 迷路に対し、ランダムでアクションを提案するpolicyを持つAgentを作成し、ゴール/禁止地点にたどり着くまでのrewardを計算してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random Agent\n",
    "class Agent():\n",
    "    # initで入力した環境のactionをランダムに出力する。\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.actions = env.actions\n",
    "\n",
    "    def policy(self, state):\n",
    "        return random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Agent gets -2.12 reward.\n",
      "Episode 1: Agent gets 0.56 reward.\n",
      "Episode 2: Agent gets -2.3600000000000003 reward.\n",
      "Episode 3: Agent gets -0.6000000000000008 reward.\n",
      "Episode 4: Agent gets -0.16000000000000036 reward.\n",
      "Episode 5: Agent gets -2.12 reward.\n",
      "Episode 6: Agent gets 0.8 reward.\n",
      "Episode 7: Agent gets -3.9200000000000017 reward.\n",
      "Episode 8: Agent gets -0.28000000000000047 reward.\n",
      "Episode 9: Agent gets 0.8 reward.\n"
     ]
    }
   ],
   "source": [
    "# Make grid environment.\n",
    "grid = [\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 9, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "env = Environment(grid)\n",
    "agent = Agent(env)\n",
    "\n",
    "# Try 10 game.\n",
    "for i in range(10):\n",
    "    # Initialize position of agent.\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.policy(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    print(\"Episode {}: Agent gets {} reward.\".format(i, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman方程式\n",
    "- 価値評価関数を網羅的に、厳密に計算する例\n",
    "- Value baseのiteration実装\n",
    "- Policy baseのiteration実装\n",
    "\n",
    "- policy baseのアクションは確率1/0しかないのか?（実装はそうなっている）　policyで複数アクションが等確率になった場合の計算方法は？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman方程式の計算(決定的)\n",
    "- 価値関数の再帰表現を用いた価値計算\n",
    "- 状態数が多くない & エピソードが長くない場合に計算可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy endゲーム(仮)\n",
    "# upかdownを繰り返し、5回行動したら終了。終了時upがHAPPY_END_BORDER以上なら\"happy_end\", そうでなければ\"bad_end\"\n",
    "# MOVE\n",
    "\n",
    "\n",
    "def V(s, gamma=0.99):\n",
    "    \"\"\" 価値関数(再帰)\n",
    "    次の状態の(遷移確率 * 価値)のmaxのみを計算に使用\n",
    "    \"\"\"\n",
    "    V = R(s) + gamma * max_V_on_next_state(s)\n",
    "    return V\n",
    "\n",
    "\n",
    "def R(s):\n",
    "    \"\"\" 報酬関数\n",
    "    終了状態のみ±1, それ以外0\n",
    "    \"\"\"\n",
    "    if s == \"happy_end\":\n",
    "        return 1\n",
    "    elif s == \"bad_end\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def max_V_on_next_state(s):\n",
    "    \"\"\" \n",
    "    全アクション, 全遷移先状態の(遷移確率 * 価値)のmaxの値を返す\n",
    "    \"\"\"\n",
    "    # If game end, expected value is 0.\n",
    "    if s in [\"happy_end\", \"bad_end\"]:\n",
    "        return 0\n",
    "\n",
    "    actions = [\"up\", \"down\"]\n",
    "    values = []\n",
    "    for a in actions:\n",
    "        transition_probs = transit_func(s, a)\n",
    "        v = 0\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            v += prob * V(next_state)\n",
    "        values.append(v)\n",
    "    return max(values)\n",
    "\n",
    "\n",
    "def transit_func(s, a):\n",
    "    \"\"\" T(s'| s, a): 状態とアクション(\"up\", \"down\")を受け取り、遷移確率を返す\n",
    "    Make next state by adding action str to state.\n",
    "    ex: (s = 'state', a = 'up') => 'state_up'\n",
    "        (s = 'state_up', a = 'down') => 'state_up_down'\n",
    "    \"\"\"\n",
    "\n",
    "    actions = s.split(\"_\")[1:]\n",
    "    LIMIT_GAME_COUNT = 5\n",
    "    HAPPY_END_BORDER = 4\n",
    "    MOVE_PROB = 0.9\n",
    "\n",
    "    def next_state(state, action):\n",
    "        return \"_\".join([state, action])\n",
    "\n",
    "    # ゲーム終了, 確率1で終了結果　に遷移\n",
    "    if len(actions) == LIMIT_GAME_COUNT:\n",
    "        up_count = sum([1 if a == \"up\" else 0 for a in actions])\n",
    "        state = \"happy_end\" if up_count >= HAPPY_END_BORDER else \"bad_end\"\n",
    "        prob = 1.0\n",
    "        return {state: prob}\n",
    "    # ゲーム続行\n",
    "    else:\n",
    "        opposite = \"up\" if a == \"down\" else \"down\"\n",
    "        return {\n",
    "            next_state(s, a): MOVE_PROB,\n",
    "            next_state(s, opposite): 1 - MOVE_PROB\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880942034605892\n",
      "0.9068026334400001\n",
      "-0.96059601\n"
     ]
    }
   ],
   "source": [
    "# 状態価値の算出\n",
    "print(V(\"state\"))\n",
    "print(V(\"state_up_up\"))\n",
    "print(V(\"state_down_down\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman方程式の計算(動的計画法近似)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## planner\n",
    "- 動的計画法による価値関数近似クラス\n",
    "- Plannerクラスをケ継承してValueIterationとPolicyIterationクラスを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Planner():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.log = []\n",
    "\n",
    "    def initialize(self):\n",
    "        self.env.reset()\n",
    "        self.log = []\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        raise Exception(\"Planner have to implements plan method.\")\n",
    "\n",
    "    def transitions_at(self, state, action):\n",
    "        transition_probs = self.env.transit_func(state, action)\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            reward, _ = self.env.reward_func(next_state)\n",
    "            yield prob, next_state, reward\n",
    "\n",
    "    def dict_to_grid(self, state_reward_dict):\n",
    "        grid = []\n",
    "        for i in range(self.env.row_length):\n",
    "            row = [0] * self.env.column_length\n",
    "            grid.append(row)\n",
    "        for s in state_reward_dict:\n",
    "            grid[s.row][s.column] = state_reward_dict[s]\n",
    "\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationPlanner(Planner):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        self.initialize()\n",
    "        actions = self.env.actions\n",
    "        V = {}\n",
    "        for s in self.env.states:\n",
    "            # Initialize each state's expected reward.\n",
    "            V[s] = 0\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            self.log.append(self.dict_to_grid(V))\n",
    "            for s in V:\n",
    "                if not self.env.can_action_at(s):\n",
    "                    continue\n",
    "                expected_rewards = []\n",
    "                for a in actions:\n",
    "                    r = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(s, a):\n",
    "                        r += prob * (reward + gamma * V[next_state])\n",
    "                    expected_rewards.append(r)\n",
    "                max_reward = max(expected_rewards)\n",
    "                delta = max(delta, abs(max_reward - V[s]))\n",
    "                V[s] = max_reward\n",
    "\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        V_grid = self.dict_to_grid(V)\n",
    "        return V_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationPlanner(Planner):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.policy = {}\n",
    "\n",
    "    def initialize(self):\n",
    "        super().initialize()\n",
    "        self.policy = {}\n",
    "        actions = self.env.actions\n",
    "        states = self.env.states\n",
    "        for s in states:\n",
    "            self.policy[s] = {}\n",
    "            for a in actions:\n",
    "                # Initialize policy.\n",
    "                # At first, each action is taken uniformly.\n",
    "                self.policy[s][a] = 1 / len(actions)\n",
    "\n",
    "    def estimate_by_policy(self, gamma, threshold):\n",
    "        V = {}\n",
    "        for s in self.env.states:\n",
    "            # Initialize each state's expected reward.\n",
    "            V[s] = 0\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in V:\n",
    "                expected_rewards = []\n",
    "                for a in self.policy[s]:\n",
    "                    action_prob = self.policy[s][a]\n",
    "                    r = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(s, a):\n",
    "                        r += action_prob * prob * \\\n",
    "                             (reward + gamma * V[next_state])\n",
    "                    expected_rewards.append(r)\n",
    "                value = sum(expected_rewards)\n",
    "                delta = max(delta, abs(value - V[s]))\n",
    "                V[s] = value\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        return V\n",
    "\n",
    "    def plan(self, gamma=0.9, threshold=0.0001):\n",
    "        self.initialize()\n",
    "        states = self.env.states\n",
    "        actions = self.env.actions\n",
    "\n",
    "        def take_max_action(action_value_dict):\n",
    "            return max(action_value_dict, key=action_value_dict.get)\n",
    "\n",
    "        while True:\n",
    "            update_stable = True\n",
    "            # Estimate expected rewards under current policy.\n",
    "            V = self.estimate_by_policy(gamma, threshold)\n",
    "            self.log.append(self.dict_to_grid(V))\n",
    "\n",
    "            for s in states:\n",
    "                # Get an action following to the current policy.\n",
    "                policy_action = take_max_action(self.policy[s])\n",
    "\n",
    "                # Compare with other actions.\n",
    "                action_rewards = {}\n",
    "                for a in actions:\n",
    "                    r = 0\n",
    "                    for prob, next_state, reward in self.transitions_at(s, a):\n",
    "                        r += prob * (reward + gamma * V[next_state])\n",
    "                    action_rewards[a] = r\n",
    "                best_action = take_max_action(action_rewards)\n",
    "                if policy_action != best_action:\n",
    "                    update_stable = False\n",
    "\n",
    "                # Update policy (set best_action prob=1, otherwise=0 (greedy))\n",
    "                for a in self.policy[s]:\n",
    "                    prob = 1 if a == best_action else 0\n",
    "                    self.policy[s][a] = prob\n",
    "\n",
    "            if update_stable:\n",
    "                # If policy isn't updated, stop iteration\n",
    "                break\n",
    "\n",
    "        # Turn dictionary to grid\n",
    "        V_grid = self.dict_to_grid(V)\n",
    "        return V_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- policy/value iterationの証明\n",
    "1. Bellman方程式の作用素表現 （value base, policy base）\n",
    "1. 関数空間V上一様ノルムを入れた際の縮小写像証明 （maxとminを計算して上としたから抑える）\n",
    "1. {T^nv}がcauchy列である証明（三角不等式と縮小写像）\n",
    "1. （存在性）(距離空間の完備性より、{T^nv}は収束列で収束先v* :=lim_{n \\to \\infty}{T^nv}が存在)\n",
    "1. Tv* =v*  の証明（「Tが縮小写像 => リプシッツ連続 => 連続関数」よりlimitの順状交換可能、ε-nから直接、など）\n",
    "1. 任意の初期値v_0に対し、v* :=lim_{n \\to \\infty}{T^nv_0}の証明（2.と5.を複数回使用することで距離が0になる）\n",
    "1. (一意性)一意性の証明もやる（v* 1, v* 2の距離をとって、Tで一回飛ばすとv* 1=v* 2以外矛盾）\n",
    "\n",
    "上記より (value/policy)iterationによる近似が正当化される\n",
    "\n",
    "関数空間Vの一様ノルムによる完備性は課題..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
