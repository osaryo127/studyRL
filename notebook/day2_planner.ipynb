{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day2 （環境から計画を立てる）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは動的計画法(Dynamic　Programing: DP)を扱う。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "sys.path.append('../../baby-steps-of-rl-ja/DP/')\n",
    "\n",
    "from environment import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day1おさらい\n",
    "\n",
    "- 強化学習はマルコフ決定過程を仮定し、その元で報酬の総和を最大化する方策をもつエージェントをもとめる問題\n",
    "    - マルコフ決定過程は以下の(S, A, T, R)の組である\n",
    "        - S: 状態全体の集合\n",
    "        - A: アクション全体の集合\n",
    "        - T: S×A×S -> [0, 1] 状態とアクションから次の状態への遷移確率を返す関数\n",
    "        - R: S×A×S -> 実数全体 状態, アクション, 遷移状態の状態から報酬を返す関数\n",
    "\n",
    "\n",
    "- Environmentクラス\n",
    "    - 迷路環境クラス\n",
    "    - State, Actionを内部に持つ\n",
    "    - Environment.step()メソッドにアクションを渡すことで、内部で遷移確率T(s'| s, a)を計算し、Tに応じた(状態遷移/rewardの計算/終了判定)をする。\n",
    "    \n",
    "- Agentクラス\n",
    "    - Environmentオブジェクトを持ち、stateに応じたアクションを返すpolicyメソッドを内部で持つ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 迷路に対し、ランダムでアクションを提案するpolicyを持つAgentを作成し、ゴール/禁止地点にたどり着くまでのrewardを計算してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random Agent\n",
    "class Agent():\n",
    "    # initで入力した環境のactionをランダムに出力する。\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.actions = env.actions\n",
    "\n",
    "    def policy(self, state):\n",
    "        return random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Agent gets -1.6 reward.\n",
      "Episode 1: Agent gets -0.5200000000000007 reward.\n",
      "Episode 2: Agent gets -1.6 reward.\n",
      "Episode 3: Agent gets -1.6 reward.\n",
      "Episode 4: Agent gets -4.8000000000000025 reward.\n",
      "Episode 5: Agent gets -2.0 reward.\n",
      "Episode 6: Agent gets -1.6400000000000001 reward.\n",
      "Episode 7: Agent gets -1.8400000000000016 reward.\n",
      "Episode 8: Agent gets -1.4 reward.\n",
      "Episode 9: Agent gets 0.1599999999999998 reward.\n"
     ]
    }
   ],
   "source": [
    "# Make grid environment.\n",
    "grid = [\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 9, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "env = Environment(grid)\n",
    "agent = Agent(env)\n",
    "\n",
    "# Try 10 game.\n",
    "for i in range(10):\n",
    "    # Initialize position of agent.\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.policy(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    print(\"Episode {}: Agent gets {} reward.\".format(i, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman方程式\n",
    "- 価値評価関数を網羅的に、厳密に計算する例\n",
    "- Value baseのiteration実装\n",
    "- Policy baseのiteration実装\n",
    "\n",
    "- policy baseのアクションは確率1/0しかないのか?（実装はそうなっている）　policyで複数アクションが等確率になった場合の計算方法は？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman方程式の計算(決定的)\n",
    "- 価値関数の再帰表現を用いた価値計算\n",
    "- 状態数が多くない & エピソードが長くない場合に計算可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy endゲーム(仮)\n",
    "# upかdownを繰り返し、5回行動したら終了。終了時upがHAPPY_END_BORDER以上なら\"happy_end\", そうでなければ\"bad_end\"\n",
    "# MOVE\n",
    "\n",
    "\n",
    "def V(s, gamma=0.99):\n",
    "    \"\"\" 価値関数(再帰)\n",
    "    次の状態の(遷移確率 * 価値)のmaxのみを計算に使用\n",
    "    \"\"\"\n",
    "    V = R(s) + gamma * max_V_on_next_state(s)\n",
    "    return V\n",
    "\n",
    "\n",
    "def R(s):\n",
    "    \"\"\" 報酬関数\n",
    "    終了状態のみ±1, それ以外0\n",
    "    \"\"\"\n",
    "    if s == \"happy_end\":\n",
    "        return 1\n",
    "    elif s == \"bad_end\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def max_V_on_next_state(s):\n",
    "    \"\"\" \n",
    "    全アクション, 全遷移先状態の(遷移確率 * 価値)のmaxの値を返す\n",
    "    \"\"\"\n",
    "    # If game end, expected value is 0.\n",
    "    if s in [\"happy_end\", \"bad_end\"]:\n",
    "        return 0\n",
    "\n",
    "    actions = [\"up\", \"down\"]\n",
    "    values = []\n",
    "    for a in actions:\n",
    "        transition_probs = transit_func(s, a)\n",
    "        v = 0\n",
    "        for next_state in transition_probs:\n",
    "            prob = transition_probs[next_state]\n",
    "            v += prob * V(next_state)\n",
    "        values.append(v)\n",
    "    return max(values)\n",
    "\n",
    "\n",
    "def transit_func(s, a):\n",
    "    \"\"\" T(s'| s, a): 状態とアクション(\"up\", \"down\")を受け取り、遷移確率を返す\n",
    "    Make next state by adding action str to state.\n",
    "    ex: (s = 'state', a = 'up') => 'state_up'\n",
    "        (s = 'state_up', a = 'down') => 'state_up_down'\n",
    "    \"\"\"\n",
    "\n",
    "    actions = s.split(\"_\")[1:]\n",
    "    LIMIT_GAME_COUNT = 5\n",
    "    HAPPY_END_BORDER = 4\n",
    "    MOVE_PROB = 0.9\n",
    "\n",
    "    def next_state(state, action):\n",
    "        return \"_\".join([state, action])\n",
    "\n",
    "    # ゲーム終了, 確率1で終了結果　に遷移\n",
    "    if len(actions) == LIMIT_GAME_COUNT:\n",
    "        up_count = sum([1 if a == \"up\" else 0 for a in actions])\n",
    "        state = \"happy_end\" if up_count >= HAPPY_END_BORDER else \"bad_end\"\n",
    "        prob = 1.0\n",
    "        return {state: prob}\n",
    "    # ゲーム続行\n",
    "    else:\n",
    "        opposite = \"up\" if a == \"down\" else \"down\"\n",
    "        return {\n",
    "            next_state(s, a): MOVE_PROB,\n",
    "            next_state(s, opposite): 1 - MOVE_PROB\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880942034605892\n",
      "0.9068026334400001\n",
      "-0.96059601\n"
     ]
    }
   ],
   "source": [
    "# 状態価値の算出\n",
    "print(V(\"state\"))\n",
    "print(V(\"state_up_up\"))\n",
    "print(V(\"state_down_down\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman方程式の計算(動的計画法近似)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- policy/value iterationの証明\n",
    "1. Bellman方程式の作用素表現 （value base, policy base）\n",
    "1. 関数空間V上一様ノルムを入れた際の縮小写像証明 （maxとminを計算して上としたから抑える）\n",
    "1. {T^nv}がcauchy列である証明（三角不等式と縮小写像）\n",
    "1. （存在性）(距離空間の完備性より、{T^nv}は収束列で収束先v* :=lim_{n \\to \\infty}{T^nv}が存在)\n",
    "1. Tv* =v*  の証明（「Tが縮小写像 => リプシッツ連続 => 連続関数」よりlimitの順状交換可能、ε-nから直接、など）\n",
    "1. 任意の初期値v_0に対し、v* :=lim_{n \\to \\infty}{T^nv_0}の証明（2.と5.を複数回使用することで距離が0になる）\n",
    "1. (一意性)一意性の証明もやる（v* 1, v* 2の距離をとって、Tで一回飛ばすとv* 1=v* 2以外矛盾）\n",
    "\n",
    "上記より (value/policy)iterationによる近似が正当化される\n",
    "\n",
    "関数空間Vの一様ノルムによる完備性は課題..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
