{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonで学ぶ強化学習　ハンズオン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day1 （強化学習の位置付けを知る）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 強化学習は学習方法の一種（抜粋）\n",
    "- 教師有り学習\n",
    "    - データと正解ラベルをセットで与える。　データが与えられたらラベルが出力されるようパラメータを調整。\n",
    "- 教師なし学習\n",
    "    - データのみを与えてデータの特徴（構造や表現）を抽出できるようパラメータを調整。\n",
    "- 強化学種\n",
    "    - 行動により報酬が得られる環境（タスク）を与えて、各状態で報酬に繋がる行動が出力される行動が出力されるようモデルのパラメータを調整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 強化学習における問題設定: Markov Deciosion Process\n",
    "\n",
    "- 強化学習における「環境」は以下の要素からなる**マルコフ決定過程（Markov Deciosion Process）**からなる。\n",
    "    - $s \\in \\mathcal{S}$ : **状態 （state）**\n",
    "    - $a \\in \\mathcal{A}$ : **行動 （action）**\n",
    "    - $T(s'\\mid s, a): \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}^{+}$ : 状態遷移の確率（**遷移関数 / Transition function**）。状態$s$と行動$a$を引数に、遷移先$s^{\\prime}$（次の状態）と遷移確率を出力する関数。\n",
    "    - $R(s, s^{\\prime}): \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ : **報酬関数 （Reward function）**。状態$s$と遷移先$s^{\\prime}$を引数に、報酬を出力する関数（行動$a$を引数に取ることもある）。    \n",
    "    \n",
    "        - メモ: $\\mathcal{S}, \\mathcal{A}$ は離散集合、連続集合どちらでもOK\n",
    "          - 離散か連続かで、適用できる強化学習アルゴリズムは変わってくる.\n",
    "          - 離散でも, 要素数が多い場合は連続と思って扱うことが多い.\n",
    "          \n",
    "\n",
    "- また、強化学習における用語を以下のように定める。\n",
    "    - **エピソード （Episode）**: 環境の開始から終了までの期間。\n",
    "    \n",
    "    - **エージェント（Agent）**: 戦略$\\pi$に従って行動する主体。（強化学習では戦略がモデルとなり、パラメーターを調節して適切な行動を出力する）\n",
    "    \n",
    "    - $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$: **戦略（policy）**。状態$s$を受け取り、行動$a$を決める関数。\n",
    "    \n",
    "    - $r:= R(s, s^{\\prime}) \\in \\mathbb{R}$ : **即時報酬（Immediate reward）**。 MDPにおける報酬。\n",
    "    \n",
    "    - $G_{t}\\in \\mathbb{R}$ : **期待報酬（Expected reward）** / **価値（Value）**。 報酬の総和を表す。\n",
    "    \n",
    "    - $\\gamma \\in (0, 1]$ :  **割引率 （Discount factor）**。 将来の報酬の影響をコントロールする変数。\n",
    "    \n",
    "    - **価値評価（Value apploximation）**: 価値を算出すること。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MDPにおける時刻$t$ から時刻$T$ までの「報酬の総和」は割引率 $\\gamma$を用いて以下のように書ける。\n",
    "\n",
    "$$G_{t} := r_{t+1} + \\gamma r_{t+2} + \\gamma^{2}r_{t+2} + \\cdots + \\gamma^{T-t-1}r_{tT} = \\sum_{k=0}^{T-t+1}\\gamma^{k}r_{t+k+1}$$\n",
    "\n",
    "- 価値$G_{t}$は時刻$t+1$での価値を用いて、再帰的に以下のように書ける。\n",
    "\n",
    "$$G_{t} = r_{t+1} + \\gamma G_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方策$\\pi$ と状態の初期値$s_0$ が与えられると、以下に示す状態と行動、報酬の確率過程が得られる。\n",
    "\n",
    "$$\n",
    "S_0, A_0, R_0, S_1, A_1, R_1, \\cdots\n",
    "$$\n",
    "\n",
    "\n",
    "- エージェントの目的は、（割引）累積価値を最大化すること。すなわち、以下の値を最大化する$\\pi$を見つけることである。\n",
    "$$\\mathbb{E}^\\pi[G_{0}] = \\mathbb{E}^\\pi\\left[\\sum_{t=0}^{T-1} \\gamma^t r_t \\right] = \\mathbb{E}^\\pi\\left[\\sum_{t=0}^{T-1} \\gamma^t  R(s_{t}, s_{t+1}) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Example(迷路)\n",
    "\n",
    "簡単な迷路における、MDPの構成要素は以下である。\n",
    "\n",
    "- $s$ 状態 : セルの位置（行/列）\n",
    "- $a$ 行動 : 上下左右への移動\n",
    "- $T$ 遷移関数: 状態と行動を受け取り、移動可能なセルとそこへの遷移確率を返す関数\n",
    "- $R$ 報酬関数: 状態を受け取り、緑(ゴール)のセルなら1、赤(ペナルティ)のセルなら-1を返す関数\n",
    "    - メモ: 迷路における報酬関数の設定は複数ある\n",
    "        - ゴール地点で+1, 進んではいけない地点で-1, それ以外は0\n",
    "        - 常に-1, 進んではいけない地点で-M （M >> 0）\n",
    "        - etc.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 位置クラス(状態$s$)、上下移動クラス（行動$a$）の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "# セル位置表現クラス\n",
    "class State:\n",
    "    def __init__(self, row: int=-1, column: int=-1):\n",
    "        self.row = row\n",
    "        self.column = column\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"State: [{self.row}, {self.column}]\"\n",
    "    \n",
    "    def clone(self):\n",
    "        return State(self.row, self.column)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.row, self.column))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.column == other.column\n",
    "    \n",
    "# アクション定義クラス\n",
    "class Action(Enum):\n",
    "    UP = 1\n",
    "    DOWN = -1\n",
    "    LEFT = 2\n",
    "    RIGHT = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State: [3, 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "State(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Action.UP: 1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Action.UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 環境実態クラス(雛形)を実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "# 迷路環境実態クラス\n",
    "class Environment:\n",
    "    def __init__(self, grid: List[List[int]], move_prob: float=0):\n",
    "        \"\"\"\n",
    "        エージェントがゴールを早く目指すように、デフォルトの報酬を負値に定義。\n",
    "        エージェントは選択した向きにmove_probの確率で移動し、(1 - move_prob)の確率で他の方向に進む。\n",
    "        \"\"\"\n",
    "        # grid is 2d-Array. Its values are treated as an attribute.\n",
    "        # kinds of attributes is following\n",
    "        # 0: ordinary cell\n",
    "        # -1: damage cell (game end)\n",
    "        # 1: reward cepp (game end)\n",
    "        # 9: block cell (can't locate agent)\n",
    "        self.grid = grid\n",
    "        self.agent_state = State()\n",
    "        self.default_reward = -0.04\n",
    "        self.move_prob = move_prob\n",
    "        self.reset()\n",
    "        \n",
    "    @property\n",
    "    def row_length(self) -> int:\n",
    "        return len(self.grid)\n",
    "    \n",
    "    @property\n",
    "    def column_length(self) -> int:\n",
    "        return len(self.grid[0])\n",
    "    \n",
    "    @property\n",
    "    def actions(self) -> List[Action]:\n",
    "        return [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]\n",
    "    \n",
    "    @property\n",
    "    def states(self) -> List[State]:\n",
    "        \"\"\"\n",
    "        迷路内の移動可能なセルを返す。(ブロックセルを除外)\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for row in range(self.row_length):\n",
    "            for col in range(self.column_lengthh):\n",
    "                if self.grid[row][col] != 9:\n",
    "                    states.append(State(row, col))\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 遷移関数$T$と報酬関数$R$の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 遷移関数$T$\n",
    "\n",
    "    - 今回のtransition_funcの実装では、actionを受け取った場合、「**非決定的**」に遷移先が決定される。\n",
    "    - 迷路の外への移動が提案された場合、その確率でその場にとどまるよう実装されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(Environment):\n",
    "    def transition_func(self, state: State, action: Action) -> Dict[State, float]:\n",
    "        \"\"\"\n",
    "        状態とアクションを受けとり、次の状態への遷移確率を返す。\n",
    "        今回の迷路では、move_probの確率で選択した方向に、\n",
    "        (1-move_prob)の確率で、選択した方向との反対以外の方向に等確率で遷移する。\n",
    "        \"\"\"\n",
    "        transition_probs = {}\n",
    "        if not self.can_action_at(state):\n",
    "            # Already on the terminal cell.\n",
    "            return transition_probs\n",
    "        \n",
    "        opposite_direction = Action(action.value * -1)\n",
    "        \n",
    "        for suggest_action in self.actions:\n",
    "            next_state = self._move(state, suggest_action)\n",
    "            \n",
    "            if suggest_action == action:\n",
    "                prob = self.move_prob\n",
    "            elif suggest_action != opposite_direction :\n",
    "                prob = (1 - self.move_prob) / 2\n",
    "            else:\n",
    "                prob = 0\n",
    "                \n",
    "            if next_state not in transition_probs:\n",
    "                transition_probs[next_state] = prob\n",
    "            else:\n",
    "                transition_probs[next_state] += prob\n",
    "\n",
    "        return transition_probs\n",
    "    \n",
    "    def can_action_at(self, state: State) -> bool:\n",
    "        \"\"\"\n",
    "        stateがアクション可能なセルかどうか判定\n",
    "        \"\"\"\n",
    "        # Indexエラー（迷路外の座標を参照）をキャッチするように実装\n",
    "        try:\n",
    "            if self.grid[state.row][state.column] == 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except IndexError as e:\n",
    "            raise ValueError(f\"This state is out of mase! {state}\")\n",
    "    \n",
    "    def _move(self, state: State, aciton) -> State:\n",
    "        \"\"\"\n",
    "        位置とアクションを受け取り、アクション可能な位置であれば、\n",
    "        受け取ったアクション方向に移動した位置に移動する。\n",
    "        移動先位置が迷路の外であれば、そのままの位置を返す。\n",
    "        \"\"\"\n",
    "        if not self.can_action_at:\n",
    "            raise ValueError(\"Can't move from here!\")\n",
    "        \n",
    "        next_state = state.clone()\n",
    "\n",
    "        # Execute an action (move).\n",
    "        if action == Action.UP:\n",
    "            next_state.row -= 1\n",
    "        elif action == Action.DOWN:\n",
    "            next_state.row += 1\n",
    "        elif action == Action.LEFT:\n",
    "            next_state.column -= 1\n",
    "        elif action == Action.RIGHT:\n",
    "            next_state.column += 1\n",
    "\n",
    "        # Check whether a state is out of the grid.\n",
    "        if not (0 <= next_state.row < self.row_length):\n",
    "            next_state = state\n",
    "        if not (0 <= next_state.column < self.column_length):\n",
    "            next_state = state\n",
    "\n",
    "        # Check whether the agent bumped a block cell.\n",
    "        if self.grid[next_state.row][next_state.column] == 9:\n",
    "            next_state = state\n",
    "\n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 報酬関数$R$\n",
    "\n",
    "    - 状態を受け取り、迷路の構造に応じた報酬関数(reward_func)を返す。\n",
    "    - 今回の設定では「緑(ゴール)のセルなら1、赤(ペナルティ)のセルなら-1」\n",
    "    \n",
    "    \n",
    "- Environment.step\n",
    "    - このメソッドでエージェントから受け取ったアクションに応じた状態遷移、即時報酬算出をする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(Environment):\n",
    "    def reward_func(self, state: State) -> (float, bool):\n",
    "        \"\"\"\n",
    "        受け取ったstateの報酬と、ゲームが終了したか否かを返す\n",
    "        デフォルトでは負の値（-0.04）を設定しているが、\n",
    "        歩き回るだけでは報酬が減る（早くゴールするよう促す）影響を与える。\n",
    "        \"\"\"\n",
    "        reward = self.default_reward\n",
    "        done = False\n",
    "\n",
    "        # Check an attribute of next state.\n",
    "        attribute = self.grid[state.row][state.column]\n",
    "        if attribute == 1:\n",
    "            # Get reward! and the game ends.\n",
    "            reward = 1\n",
    "            done = True\n",
    "        elif attribute == -1:\n",
    "            # Get damage! and the game ends.\n",
    "            reward = -1\n",
    "            done = True\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def reset(self) -> State:\n",
    "        # Locate the agent at lower left corner.\n",
    "        self.agent_state = State(self.row_length - 1, 0)\n",
    "        return self.agent_state\n",
    "\n",
    "    def step(self, action) -> (State, float, bool):\n",
    "        \"\"\"\n",
    "        現在のエージェントの状態にアクションを適用し、遷移先状態, 即時報酬, 終了判定を返す\n",
    "        \"\"\"\n",
    "        next_state, reward, done = self.transit(self.agent_state, action)\n",
    "        if next_state is not None:\n",
    "            self.agent_state = next_state\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def transit(self, state: State, action: Action) -> (State, float, bool):\n",
    "        \"\"\"\n",
    "        状態とアクションを受け取り、遷移関数によって遷移確率を算出、。\n",
    "        確率に応じた繊維を実行し、遷移先状態、即時報酬、終了判定を返す。\n",
    "        \"\"\"\n",
    "        transition_probs = self.transition_func(state, action)\n",
    "        if len(transition_probs) == 0:\n",
    "            return None, None, True\n",
    "\n",
    "        next_states = []\n",
    "        probs = []\n",
    "        for s in transition_probs:\n",
    "            next_states.append(s)\n",
    "            probs.append(transition_probs[s])\n",
    "\n",
    "        next_state = np.random.choice(next_states, p=probs)\n",
    "        reward, done = self.reward_func(next_state)\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Demonstrate (エージェントを動かしてみる)\n",
    "\n",
    "- 方策（policy）に従って移動するエージェントを実装する。\n",
    "    - 今回はランダム移動\n",
    "    \n",
    "    \n",
    "- Environment(環境)を作成し、ループで10回迷路を探索する。\n",
    "- agent.policyによりactionが選択され、env.stepからアクションactionに応じた遷移先(next_state)と即時報酬(reward)を得る。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env: Environment):\n",
    "        \"\"\"\n",
    "        エージェントの初期化\n",
    "        \"\"\"\n",
    "        self.actions = env.actions\n",
    "        \n",
    "    def policy(self, state: State) -> Action:\n",
    "        \"\"\"\n",
    "        状態を受け取り、アクションを決定する。\n",
    "        \"\"\"\n",
    "        return random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrate\n",
    "grid = [\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 9, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "env = Environment(grid)\n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Agent gets 0.4800000000000001 reward.\n",
      "Episode 1, Agent gets -1.7200000000000002 reward.\n",
      "Episode 2, Agent gets -1.8400000000000003 reward.\n",
      "Episode 3, Agent gets -3.280000000000001 reward.\n",
      "Episode 4, Agent gets -1.56 reward.\n",
      "Episode 5, Agent gets -3.120000000000001 reward.\n",
      "Episode 6, Agent gets 0.6000000000000001 reward.\n",
      "Episode 7, Agent gets -1.32 reward.\n",
      "Episode 8, Agent gets 0.0399999999999997 reward.\n",
      "Episode 9, Agent gets 0.19999999999999984 reward.\n"
     ]
    }
   ],
   "source": [
    "# 環境はagentをattributeとしてはも持っていない\n",
    "# テキストの実装、next_stateだったりをメソッドの戻り値として定義する必要性はある？\n",
    "# doneもenvの@propertyで env.is_terminalとかで定義すればよくない？ （agent_stateの位置のgridの値を参照すれば良い）　\n",
    "# -> そうすればEpisode内でnext_state保存する必要もdoneを置く必要もないので簡潔になる。\n",
    "for i in range(10):\n",
    "    # Initialize position of agent.\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.policy(env.agent_state)\n",
    "        _, reward, done =  env.step(action)\n",
    "        if reward:\n",
    "            total_reward += reward\n",
    "        \n",
    "    print(f\"Episode {i}, Agent gets {total_reward} reward.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 色々いじって遊ぼう！\n",
    "- transit_funcやreward_funcを変えてみる\n",
    "- agentの足跡を辿れるようにする\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
